# -*- coding: utf-8 -*-
"""cnn_competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlKuQW5-FYoC2479CWCPfcoE87OIuJ4p
"""

#*********************************
'''
 Classifier: Convolutional Neural Network using Keras

 Dataset: CIFAR-10
    INFO: https://www.cs.toronto.edu/~kriz/cifar.html
 Author:  Manuel Serna-Aguilera
 This program was run using Google Colaboratory
'''
#*********************************

# Setup: libraries
import keras
from keras.datasets import cifar10
from keras.layers import BatchNormalization
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.models import Sequential
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import sys

# Augment data to further improve accuracy
data_gen = ImageDataGenerator(
    width_shift_range=0.1, # shift image horizontally
    height_shift_range=0.1, # shift image vertically
    zoom_range=0.2, # zoom in as close as 1.2x or out as far as 0.8
    shear_range=0.1,
    rotation_range=10 # rotate image in degrees
)

# Setup: vars
num_classes = 10
in_shape = (32,32,3)

# Get & preprocess training & test data + labels from keras 
def get_cifar_data():
	# Load from Keras
	(x_train, y_train), (x_test, y_test) = cifar10.load_data()

	# One-hot encode labels
	y_train = to_categorical(y_train)
	y_test = to_categorical(y_test)

	# Convert image data to floating-pt.
	x_train = x_train.astype('float32')
	x_test = x_test.astype('float32')

	# Normalize color values
	x_train = x_train/255.0
	x_test = x_test/255.0

	return x_train, y_train, x_test, y_test

#---------------------------------
'''
Custom CNN model based on VGG architecture adapted for much smaller CIFAR10 images
Input:
    num_classes: size of output layer or number of true labels
    in_shape: shape of training data (in this case it's (32x32x3))
Architecture:
NOTE: Added the following pre-processing to data itself
    1. Data augmentation: change images so model has more examples to work with and
is able to better understand the features we need it to learn.
NOTE: I also included batch normalization (mean-center data) after (almost) each layer

VGG-like architecture
    block 1: 
        conv: 32 3x3 filters, relu
        batch normalization layer
        conv: 32 3x3 filters, relu
        batch normalization layer
        max pooling
        dropout of 20%
    block 2: 
        conv: 64 3x3 filters, relu
        batch normalization layer
        conv: 64 3x3 filters, relu
        batch normalization layer
        max pooling
        dropout of 30%
    block 3: 
        conv: 128 3x3 filters, relu
        batch normalization layer
        conv: 128 3x3 filters, relu
        batch normalization layer
        max pooling
        dropout of 40%
    fully-connected: 
        128 activations, relu
        batch normalization layer
        128 activations, relu
        batch normalization layer
        dropout of 50%
    output:
        10 outputs, softmax

* Final testing accuracy: 87.19%, trained on 75 epochs with batch size of 64
'''
#---------------------------------
def get_cifar10_model(num_classes, in_shape):
    model = Sequential()

    # Block 1: 
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=in_shape))
    model.add(BatchNormalization())
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))

    # Block 2: 
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.3))

    # Block 3:
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.4))
    model.add(Flatten())

    # Fully-connected:
    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.50))
    model.add(Dense(num_classes, activation='softmax'))

    # Compile using ADAM optimizer
    model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Load training-ready cifar-10 data
x_train, y_train, x_test, y_test = get_cifar_data()

# Get def of model and fit to training data; use test data as eval data
model = get_cifar10_model(num_classes, in_shape)
#history = model.fit(x_train, y_train, epochs=60, batch_size=64, validation_data=(x_test, y_test), verbose=1)

# TODO: train on new model below
history = model.fit(data_gen.flow(x_train, y_train, batch_size=64), epochs=40, validation_data=(x_test, y_test), verbose=1)

# Evaluate model on test data
score = model.evaluate(x_test, y_test, verbose=0)
print('Test accuracy: ', score[1])

# Plot the loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('Loss')
plt.xlabel('epoch')

# Plot the model accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training', 'validation'])
plt.title('Accuracy')
plt.xlabel('epoch')

# Print all individual test classifications to file.
#  Specifically, print index of classes (0-9)
n_test = y_test.shape[0]
predictions_file = "model_predictions.txt"

with open(predictions_file, 'w') as file:
    for t_sample in range(0, n_test):
        print(t_sample)
        prediction = model.predict(np.array([x_test[t_sample], ]))[0]
        # Record: label prediction
        file.write('{}    {}\n'.format(np.argmax(y_test[t_sample]), np.argmax(prediction)))

# In Colaboratory, specify that we want to download our files
from google.colab import files
files.download(predictions_file)